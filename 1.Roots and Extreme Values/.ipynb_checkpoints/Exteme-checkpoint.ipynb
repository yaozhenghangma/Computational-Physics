{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extremes\n",
    "\n",
    "Normally, there are two ways to get extremes of a function.\n",
    "\n",
    "1. find the root of equation $\\frac{\\partial f}{\\partial x}=0$\n",
    "2. approach local extreme step by step\n",
    "\n",
    "Since the first method has been discussed in the note of finding roots, we only consider the second method here.\n",
    "\n",
    "In machine learning, how to find the extremes is a core topic. Thus, with the development of machine learning, a lot of methods have been created, e.g. OGD, SGD, momenum, Adam and so on. Here, we only talk about ordinary gradient descent(OGD).\n",
    "\n",
    "### Steepest-Descent Method\n",
    "\n",
    "As the gradient of a function points to the steepest descent direction, we can move x toward this direction to find the local minimum.\n",
    "\n",
    "$$x_{i+1} = x_i -\\alpha\\nabla f(x_i)$$\n",
    "\n",
    "where $\\alpha$ is learning rate. That is a hyperparameter set manually.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Let's find the minimum of function $f(x, y) = x^2+y^2$\n",
    "\n",
    "#### Code Example (Julia language):\n",
    "\n",
    "In machine learning, we uaually use automatic differentiation to get the differentiation of a function, e.g. Zygote package of Julia. But, we use its differentiation directly here.\n",
    "\n",
    "$$\\nabla f(x,y) = 2x\\vec{e_x}+2y\\vec{e_y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 0.00014094409376036872\n",
      "y: 0.0015717102227927895\n",
      "Function value: 2.4901382619972914e-6\n"
     ]
    }
   ],
   "source": [
    "# define function and derived function\n",
    "function func(x::Float64, y::Float64)\n",
    "    return x^2+y^2\n",
    "end\n",
    "\n",
    "function derivedFunc(x::Float64, y::Float64)\n",
    "    return 2x, 2y\n",
    "end\n",
    "\n",
    "# define variables\n",
    "lr = 0.001  # learning rate\n",
    "ϵ = 1.0e-8  # tolerance\n",
    "x = rand()  # initial x\n",
    "y = rand()  # initial y\n",
    "\n",
    "# loop to find minimum\n",
    "while true\n",
    "    originalValue = func(x, y)\n",
    "    dx, dy = derivedFunc(x, y)\n",
    "    x -= lr * dx\n",
    "    y -= lr * dy\n",
    "    newValue = func(x, y)\n",
    "    \n",
    "    if abs(newValue-originalValue) < ϵ\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"x: \", x)\n",
    "println(\"y: \", y)\n",
    "println(\"Function value: \", func(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
